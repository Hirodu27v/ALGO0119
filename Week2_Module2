前回のビデオでは、自動コンテンツがさまざまな報道機関でどのように使用されているか、いくつかの事例を紹介しました。
今日は、自動コンテンツの利点と限界をもう少し具体的に説明します。
これらのテクノロジやアプローチをあなたの組織に組み入れるべきかどうか判断する助けになるはずです。

スピード、スケール、正確さ、そしてパーソナライズ。これらは自動コンテンツの四つの主な利点です。
スピードの面では、コンピュータは多くのタスクを人間よりも速く処理できるという事実に議論の余地はあまりありません。
金融やニュース速報など、一部の分野ではスピードが全てといってよいでしょう。
スピード上の利点が大きい自動化システムが財務情報へのアクセスや多額の資金が失われる危険を抑止するために運用されていることはよく知られています。

自動化にはスケールメリットもあります。
最小限の作業で同じ種類のイベントを繰り返し処理できるようにすれば、地域や時間が異なるコンテンツにもアルゴリズムの適用範囲を拡大できます。
時間をまたいでスケールすることは、他の方法では不可能に思えるほど取材の一貫性を保てるので、新しい取材の端緒も生まれます。
自動化のもう一つの利点は精度の高さです。システムのセットアップとデバッグが完了すると、テンプレートとルールを適用することによって一貫性が保たれます。
自動化によって間違いが撲滅されるわけではありませんが、人間と同じようなエラーはなくなります。

パーソナライゼーションとは、年齢、性別、住所、最近興味がある話題など、ユーザーの特性にコンテンツを適合させることです。
自動化は、コンテンツを個人に適合させる新しい機会を提供するのです。
それでは、具体的な例を挙げながらもう少し詳しく説明しましょう。 

スピードという面の好例はロサンゼルス・タイムズです。
LA Times QuakeBotは、米地質調査所がマグニチュード3.0以上の地震のデータを提供して数分以内に、地図付きのストーリースタブを作成しています。
スピードは安全に直結します。人々は震源が自分自身や愛する人たちの近くなのかどうかできるだけ早く知りたいと思うはずです。

自動化のもう一つの利点は規模でしたね。
前回のビデオで、2016年の選挙におけるワシントンポストの報道を紹介しました。
ワシントンポストは彼らの2016年の報道で大統領選、34全ての上院議員選、12全てのの州知事選をカバーすることができました。
2012年には下院議員選の15％だけしかカバーできなかったのに、2016年には100％に達したのです。
これらの選挙をめぐる利用可能なデータを持っていたので、彼らは全てをカバーするまでに報道範囲を拡大することができました。

精度も自動化の利点に数えられます。例えばAP通信では、企業収益の報道に自動化を導入した後、エラー率が低下しました。
自動化技術はタイプミスや、計算ミスをしません。ただしそれは、人間がしないような間違いをしないという意味ではないことに注意が必要です。
ここで、思わぬエラーを引き起こすかもしれない例をお見せしたいと思います。文脈を理解しないアルゴリズムはどんな影響を及ぼすでしょう。
アルゴリズムはこんな文章を作り出します。
「Move 社は昨日、安値を更新しました。注目すべきです。始値は0.00ドルでしたが、52週間ぶりの安値である0.00ドルに下落しました」
明らかに無意味な文章です。システムは同社が上場廃止になっていて、上場廃止された会社の株価はゼロとして表示されるという重要な文脈を見逃しました。
システムは「上場廃止」の概念を理解していないので、無意味な文章を生成してしまったのです。
            
最後はパーソナライズについてです。
自動化技術は記事に付属する図表が、年齢や場所などのユーザーの特性に基づいて調整される全く新しい可能性を開拓しつつあります。
ニューヨークタイムズは記事の読者がいる地域にローカライズされた地図とテキストを挿入したことがあります。
記事のページにアクセスすると、IPアドレスからあなたの居場所を特定し、地図とテキストを自動的に調整します。
この種のプレゼンテーションは、ユーザー一人一人との関連性を高めます。
個人的な意見ですが、パーソナライズされた記事の生成を試してみる価値はとても大きいと思います。

それでは、自動化コンテンツを制限するもののうち、４点を説明しましょう。
データアクセスは、自動コンテンツを制限する最大のものと言えます。
データ駆動型テンプレートの動作には、構造化されたデータと知識ベースが必要です。
利用可能なデータの質や豊富さは、あなたの自動化されたコンテンツの質に直結しています。
ここでもデータは競争上の差別化要因となります。
データを独占すれば、コンテンツも独占できます。報道機関は、独自のデータ資産を取得することを、綿密に検討すべきです。
具体的には、文書のデジタル化、公的記録の開示請求、あるいはデータを収集するためにセンサージャーナリズムのようなことに取り組むこともできるでしょう。
私が「Why and how」と呼んでいる制限要因もあります。出来事がなぜ、そしてどのように起こるのかを説明するには、高いレベルの認識と解釈能力が必要です。
また、これらの説明を行う上では人間社会を理解することが必要になることもしばしばあるのです。
アルゴリズムと自動化されたコンテンツのシステムは社会への理解や文化的解釈、ジャーナリズムの常識を備えていないので、演劇批評などは最も苦手な分野かもしれません。

法的知識もありませんから、記事を自動で執筆するシステムが誰かの評判を傷つけるような文章を生成した場合、名誉毀損が成立する場合があります。
今までに述べてきたような理由から、人間がコンテンツを公開前にチェックしています。

最後の制限は、これらのシステムの記事の質であり、特にテンプレートベースのアプローチに当てはまります。
テンプレートベースのアプローチは質にばらつきがないものの、複数のコンテンツを消費するユーザーが飽きてしまう可能性があります。
統計的手法によるコンテンツはより洗練されていますが、一方で誤った内容のものを生み出す可能性もあります。
これらの制限を回避する方法の一つは、人間をプロセスに関与させることです。
これもまた、自動化プロセスと人間を互いに補完するというアイデアです。

その実例はどのようなものでしょうか。
If / Then / Elseルールによって制御されたテンプレートを使って執筆できる画期的なワードプロセッサが既に登場しています。
次のビデオでは、Arria Studioと呼ばれるこのインターフェースを実演し、データ駆動型テンプレートを作成する方法をご紹介します。
